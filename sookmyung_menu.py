import requests
import csv
import json
import time

# ì¹´ì¹´ì˜¤ API ì„¤ì •
API_KEY = {API_KEY}
headers = {"Authorization": f"KakaoAK {API_KEY}"}

# ì²­íŒŒë™ ì¤‘ì‹¬ ì¢Œí‘œ
x, y = 126.97220831510235, 37.545505260572455

# ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
queries = ["ìˆ ì§‘", "í˜¸í”„", "ì‹ë‹¹", "ë§›ì§‘", "ì¹´í˜", "ì£¼ì ", "í¬ì°¨", "ì–‘ì‹", "í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "í“¨ì „", "ì™€ì¸ë°”", "í", "ë§¥ì£¼"]

# í‚¤ì›Œë“œ ê²€ìƒ‰ í•¨ìˆ˜
def search_keyword_places(query, x, y, radius=3000, page=1):
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    params = {
        "query": query,
        "x": x,
        "y": y,
        "radius": radius,
        "page": page,
        "size": 15
    }
    res = requests.get(url, headers=headers, params=params)
    print(f"[DEBUG] {query} - Page {page} - status: {res.status_code}")
    try:
        return res.json()
    except Exception as e:
        print("[ERROR] JSON ë””ì½”ë”© ì‹¤íŒ¨:", e)
        return {}

# í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_keyword_places(queries, x, y, radius=3000):
    collected = []
    seen_ids = set()
    
    for query in queries:
        for page in range(1, 100):
            result = search_keyword_places(query, x, y, radius, page)
            documents = result.get("documents", [])
            print(f"ğŸ“„ {query} - Page {page} - ìˆ˜ì‹ ëœ ê°€ê²Œ ìˆ˜: {len(documents)}")
            if not documents:
                break
            for place in documents:
                pid = place.get("id")
                if pid not in seen_ids:
                    seen_ids.add(pid)
                    collected.append(place)
            time.sleep(0.3)  # ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€ (rate limit)
    
    return collected

# âœ… CSV ì €ì¥
def save_to_csv(data, filename="ê°ˆì›”ë™_ìˆ ì§‘_ì‹ë‹¹_í†µí•©.csv"):
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["name", "address", "phone", "category", "lat", "lng", "id", "url"])
        for place in data:
            writer.writerow([
                place.get('place_name', ''),
                place.get('address_name', ''),
                place.get('phone', ''),
                place.get('category_name', ''),
                place.get('y', ''),
                place.get('x', ''),
                place.get('id', ''),
                place.get('place_url', '')
            ])

# JSON ì €ì¥
def save_to_json(data, filename="ê°ˆì›”ë™_ìˆ ì§‘_ì‹ë‹¹_í†µí•©.json"):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

# ì‹¤í–‰
if __name__ == "__main__":
    print(f"ğŸ” ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ê°ˆì›”ë™ ì‹ë‹¹ ë° ìˆ ì§‘ ìˆ˜ì§‘ ì¤‘...")
    places = crawl_keyword_places(queries, x, y, radius=3000)
    print(f"ì´ ìˆ˜ì§‘ëœ ê°€ê²Œ ìˆ˜: {len(places)}ê°œ")

    save_to_csv(places)
    save_to_json(places)
    print("ì €ì¥ ì™„ë£Œ! (CSV & JSON)")
